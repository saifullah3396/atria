{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import dotenv\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"mytestuser1\"\n",
    "PASSWORD = \"mytestpassword\"\n",
    "EMAIL = \"mytestuser1@gmail.com\"\n",
    "FULL_NAME = \"My Test User\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we sign up to the atria hub to manage our datasets and models\n",
    "from atria_hub.hub import AtriaHub\n",
    "hub = AtriaHub()\n",
    "try:\n",
    "    user = hub.auth.sign_up(email=EMAIL, password=PASSWORD, full_name=FULL_NAME, username=USERNAME)\n",
    "except Exception:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sign-in successful for mytestuser1@gmail.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<atria_hub.hub.AtriaHub at 0x7ed9794ea750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after a successful sign up, we can initialize the hub with our login credentials\n",
    "user = hub.auth.sign_in(email=EMAIL, password=PASSWORD)\n",
    "hub.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-19 17:13:51][atria_datasets.core.dataset.atria_dataset][INFO] Loading dataset cifar10 from registry.\n",
      "[2025-07-19 17:13:51][atria_datasets.core.dataset.atria_dataset][INFO] Loading cached split train from /mnt/hephaistos/.atria/datasets/cifar10/main-01eefb81/delta/train\n",
      "[2025-07-19 17:13:52][atria_datasets.core.dataset.atria_dataset][INFO] Loading cached split test from /mnt/hephaistos/.atria/datasets/cifar10/main-01eefb81/delta/test\n",
      "[2025-07-19 17:13:52][atria_datasets.core.dataset.atria_dataset][WARNING] Split validation not found. Skipping.\n",
      "[2025-07-19 17:13:52][atria_datasets.core.dataset.atria_dataset][INFO] Uploading dataset Cifar10 to hub with name test-cifar10-5 and config test.\n",
      "[2025-07-19 17:13:52][atria_datasets.core.dataset.atria_dataset][ERROR] Failed to upload dataset to hub: This dataset already contains uploaded files in branch 'test'. Please use a different branch name or delete the existing files.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This dataset already contains uploaded files in branch 'test'. Please use a different branch name or delete the existing files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m dataset = AtriaImageDataset.load_from_registry(\u001b[33m\"\u001b[39m\u001b[33mcifar10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# now lets upload the dataset to the atria hub\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest-cifar10-5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hephaistos/projects/atria_agent/atria/atria_datasets/src/atria_datasets/core/dataset/atria_dataset.py:487\u001b[39m, in \u001b[36mAtriaDataset.upload_to_hub\u001b[39m\u001b[34m(self, name, branch, is_public)\u001b[39m\n\u001b[32m    478\u001b[39m     hub = AtriaHub().initialize()\n\u001b[32m    479\u001b[39m     dataset = hub.datasets.get_or_create(\n\u001b[32m    480\u001b[39m         username=hub.auth.username,\n\u001b[32m    481\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m         is_public=is_public,\n\u001b[32m    486\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_files\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset_files_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    494\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33matria_hub\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is required to upload datasets to the hub. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it using \u001b[39m\u001b[33m'\u001b[39m\u001b[33muv add https://github.com/saifullah3396/atria_hub\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hephaistos/projects/atria_agent/atria/atria_hub/src/atria_hub/api/datasets.py:110\u001b[39m, in \u001b[36mDatasetsApi.upload_files\u001b[39m\u001b[34m(self, dataset, branch, dataset_files)\u001b[39m\n\u001b[32m    108\u001b[39m dir_ls = \u001b[38;5;28mself\u001b[39m._client.fs.ls(tgt, refresh=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mdelta/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [x[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dir_ls]:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    111\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis dataset already contains uploaded files in branch \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use a different branch name or delete the existing files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# iterate over the dataset files and upload them to the hub\u001b[39;00m\n\u001b[32m    116\u001b[39m logger.info(\n\u001b[32m    117\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files to dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in branch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: This dataset already contains uploaded files in branch 'test'. Please use a different branch name or delete the existing files."
     ]
    }
   ],
   "source": [
    "# now we first load the cifar10 dataset from the atria datasets predefiend registry\n",
    "from atria_datasets import AtriaImageDataset\n",
    "dataset = AtriaImageDataset.load_from_registry(\"cifar10\")\n",
    "# now lets upload the dataset to the atria hub\n",
    "dataset.upload_to_hub('test-cifar10-5', branch='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-19 17:06:34][atria_datasets.core.dataset.atria_dataset][INFO] Uploading dataset Cifar10 to hub with name test-cifar10-4 and config test.\n",
      "[2025-07-19 17:06:35][atria_datasets.core.dataset.atria_dataset][ERROR] Failed to upload dataset to hub: This dataset already contains uploaded files in branch 'test'. Please use a different branch name or delete the existing files.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This dataset already contains uploaded files in branch 'test'. Please use a different branch name or delete the existing files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# now lets upload the dataset to the atria hub\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest-cifar10-4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hephaistos/projects/atria_agent/atria/atria_datasets/src/atria_datasets/core/dataset/atria_dataset.py:487\u001b[39m, in \u001b[36mAtriaDataset.upload_to_hub\u001b[39m\u001b[34m(self, name, branch, is_public)\u001b[39m\n\u001b[32m    478\u001b[39m     hub = AtriaHub().initialize()\n\u001b[32m    479\u001b[39m     dataset = hub.datasets.get_or_create(\n\u001b[32m    480\u001b[39m         username=hub.auth.username,\n\u001b[32m    481\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m         is_public=is_public,\n\u001b[32m    486\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_files\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset_files_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    494\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33matria_hub\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is required to upload datasets to the hub. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it using \u001b[39m\u001b[33m'\u001b[39m\u001b[33muv add https://github.com/saifullah3396/atria_hub\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hephaistos/projects/atria_agent/atria/atria_hub/src/atria_hub/api/datasets.py:110\u001b[39m, in \u001b[36mDatasetsApi.upload_files\u001b[39m\u001b[34m(self, dataset, branch, dataset_files)\u001b[39m\n\u001b[32m    108\u001b[39m dir_ls = \u001b[38;5;28mself\u001b[39m._client.fs.ls(tgt, refresh=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mdelta/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [x[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dir_ls]:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    111\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis dataset already contains uploaded files in branch \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use a different branch name or delete the existing files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m     )\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# iterate over the dataset files and upload them to the hub\u001b[39;00m\n\u001b[32m    116\u001b[39m logger.info(\n\u001b[32m    117\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files to dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in branch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: This dataset already contains uploaded files in branch 'test'. Please use a different branch name or delete the existing files."
     ]
    }
   ],
   "source": [
    "# now lets upload the dataset to the atria hub\n",
    "dataset.upload_to_hub('test-cifar10-4', branch='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-19 17:06:39][atria_datasets.core.dataset.atria_hub_dataset][INFO] Streaming dataset split train from lakefs://mytestuser1-26c51e41f3db44e1/test/delta/train/\n",
      "[2025-07-19 17:06:39][atria_datasets.core.dataset.atria_hub_dataset][INFO] Streaming dataset split test from lakefs://mytestuser1-26c51e41f3db44e1/test/delta/test/\n",
      "[2025-07-19 17:06:39][atria_datasets.core.dataset.atria_dataset][WARNING] Split validation not found. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homepage=None description='CIFAR-10 dataset' license=None citation=None dataset_labels=DatasetLabels(\n",
      "    classification=[\n",
      "        'airplane',\n",
      "        'automobile',\n",
      "        'bird',\n",
      "        'cat',\n",
      "        'deer',\n",
      "        'dog',\n",
      "        'frog',\n",
      "        'horse',\n",
      "        'ship',\n",
      "        'truck'\n",
      "    ],\n",
      "    ser=None,\n",
      "    layout=None\n",
      ")\n",
      "ImageInstance(\n",
      "    index=0,\n",
      "    sample_id='09447729-0f39-4fb3-855f-9aedf183d106',\n",
      "    image=Image(\n",
      "        content=<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x71FBDD968050>,\n",
      "        source_width=32,\n",
      "        source_height=32,\n",
      "        width=32,\n",
      "        height=32,\n",
      "        channels=3\n",
      "    ),\n",
      "    gt=GroundTruth(\n",
      "        classification=ClassificationGT(label=Label(value=6, name='frog'))\n",
      "    )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# now lets upload the dataset to the atria hub\n",
    "from atria_datasets.core.dataset.atria_dataset import AtriaImageDataset\n",
    "dataset = AtriaImageDataset.load_from_hub(\n",
    "    name=f\"{hub.auth.username}/test-cifar10-4/test\", streaming=True\n",
    ")\n",
    "print(dataset.metadata)\n",
    "x = dataset.train[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hephaistos/projects/atria_agent/atria/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "[2025-07-19 17:14:09][atria_models.core.timm_model][INFO] Setting up model TimmModel(model_name='resnet50', num_classes=10)\n"
     ]
    }
   ],
   "source": [
    "# now we build a resnet50 model pipeline for image classification from the atria models registry\n",
    "from atria_models.pipelines.atria_model_pipeline import AtriaModelPipeline\n",
    "model_pipeline = AtriaModelPipeline.load_from_registry(\n",
    "    pipeline_name=\"image_classification\",\n",
    "    model_name=\"resnet50\",\n",
    "    dataset_metadata=dataset.metadata,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-19 17:14:11][atria_models.pipelines.atria_model_pipeline][INFO] Uploading model ImageClassificationPipeline to hub with name resnet50 and config main.\n"
     ]
    }
   ],
   "source": [
    "# now we upload the model pipeline to the atria hub\n",
    "model_pipeline.upload_to_hub(\n",
    "    name=\"resnet50\",\n",
    "    branch=\"main\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-19 17:14:51][atria_models.pipelines.atria_model_pipeline][INFO] Loading model mytestuser1/resnet50/main from hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mytestuser1/resnet50/main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-19 17:14:52][atria_models.core.timm_model][INFO] Setting up model TimmModel(model_name='resnet50', num_classes=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassificationPipeline:\n",
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "ModuleDict                                         --\n",
      "├─TimmModel: 1-1                                   --\n",
      "│    └─ResNet: 2-1                                 --\n",
      "│    │    └─Conv2d: 3-1                            9,408\n",
      "│    │    └─BatchNorm2d: 3-2                       128\n",
      "│    │    └─ReLU: 3-3                              --\n",
      "│    │    └─MaxPool2d: 3-4                         --\n",
      "│    │    └─Sequential: 3-5                        215,808\n",
      "│    │    └─Sequential: 3-6                        1,219,584\n",
      "│    │    └─Sequential: 3-7                        7,098,368\n",
      "│    │    └─Sequential: 3-8                        14,964,736\n",
      "│    │    └─SelectAdaptivePool2d: 3-9              --\n",
      "│    │    └─Linear: 3-10                           20,490\n",
      "├─CrossEntropyLoss: 1-2                            --\n",
      "├─CrossEntropyLoss: 1-3                            --\n",
      "===========================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# now we load the model pipeline from the hub\n",
    "name = f\"{hub.auth.username}/resnet50/main\"\n",
    "print(name)\n",
    "model_pipeline = AtriaModelPipeline.load_from_hub(\n",
    "    name=f\"{hub.auth.username}/resnet50/main\",\n",
    ")\n",
    "print(model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 2, 2, 0, 9, 1, 6, 2, 2, 3]\n",
      "[7, 7, 2, 7, 2, 0, 8, 0, 7, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassificationModelOutput(loss=tensor(2.3897, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0768, -0.3727,  0.1541, -0.0527, -0.4946,  0.3047,  0.2774,  0.1382,\n",
       "         -0.0338,  0.0631]], grad_fn=<AddmmBackward0>), prediction=None, label=Label(value=tensor([3]), name=['light']))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from atria_core.types.factory import ImageInstanceFactory\n",
    "from atria_core.types import GroundTruth\n",
    "model_pipeline.enable_runtime_transforms()\n",
    "image = ImageInstanceFactory.build()\n",
    "image.gt = GroundTruth(classification = image.gt.classification)\n",
    "model_pipeline.training_step([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import OmegaConf\n",
    "# print(json.dumps(OmegaConf.to_container(model_pipeline.config), indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from atria_models.pipelines.classification.image import ImageClassificationPipeline\n",
    "\n",
    "# print(model_pipeline.config)\n",
    "# model_pipeline.upload_to_hub(\n",
    "#     \"mymodel\"\n",
    "# )\n",
    "\n",
    "# loaded = ImageClassificationPipeline.load_from_hub(\n",
    "#     \"mymodel\", \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded.state_dict()['dataset_metadata'] = model_pipeline.state_dict()['dataset_metadata']\n",
    "# loaded.state_dict()['config'] = model_pipeline.state_dict()['config']\n",
    "# print(loaded.state_dict()['config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from atria_models.pipelines.classification.image import ImageClassificationPipeline\n",
    "# from omegaconf import OmegaConf\n",
    "# from atria_models.registry import MODEL\n",
    "# from atria_models.core.timm_model import TimmModel\n",
    "# from rich.pretty import pretty_repr\n",
    "# import json\n",
    "# model = TimmModel(model_name=\"resnet50\")\n",
    "# pipeline = ImageClassificationPipeline(model=model, checkpoint_configs='test')\n",
    "# print(json.dumps(pipeline._config, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
